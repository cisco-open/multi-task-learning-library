{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the project directory here to find the emtl package\n",
    "import os, sys\n",
    "project_dir = os.path.abspath('..')\n",
    "\n",
    "# if the kernel wasn't restarted, the folder might still be there\n",
    "if project_dir not in sys.path: \n",
    "    sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models as M\n",
    "from torchvision import datasets as D\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# EMTL Library Imports\n",
    "from emtl import Task, Trainer\n",
    "from emtl.algorithms import SequentialTraining\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabHeadPipeline(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, device: str = 'cpu') -> None:\n",
    "        super().__init__()\n",
    "        self.head = M.segmentation.deeplabv3.DeepLabHead(\n",
    "            in_channels=in_features, num_classes=out_features).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, original_shape: tuple[int, int]) -> dict[str, torch.Tensor]:\n",
    "        x = self.head(x)\n",
    "        x = torch.nn.functional.interpolate(x, size=original_shape, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a ResNet50 backbone removing the last two layers (fc and avgpool)\n",
    "backbone = M.resnet50(replace_stride_with_dilation=[False, True, True]).to(device)\n",
    "backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "head = DeepLabHeadPipeline(in_features=2048, out_features=21, device=device)\n",
    "\n",
    "shape = (520,520)\n",
    "tinput  = T.Compose([T.Resize(shape), T.ToTensor()])\n",
    "ttarget = lambda x : T.Compose([T.Resize(shape), T.PILToTensor()])(x).squeeze(0).long()\n",
    "testset = D.VOCSegmentation(root='data', image_set='val', transform=tinput, target_transform=ttarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_segmentation_task = Task(\n",
    "    name = 'voc_seg',\n",
    "    head = head,\n",
    "    trainset = testset, \n",
    "    testset = testset,\n",
    "    dataloader_params = {'batch_size': 4, 'num_workers': 4, 'pin_memory': True, 'drop_last': True},\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=255),\n",
    "    optimizer_fn = torch.optim.Adam,\n",
    "    scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    backbone = backbone,\n",
    "    tasks = [VOC_segmentation_task],\n",
    "    algorithm = SequentialTraining(epochs=5),\n",
    "    config='config.ini'\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52372b67cea419ab21537734b4cd425ac02e0aecadbd8297b2ce6404efbcdf36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
